name: Deploy Biomage infrastructure on AWS
on:
  workflow_dispatch:
    inputs:
      workflow_actions:
        type: choice
        description: Select actions to perform
        options:
        - deploy and configure
        - configure
        default: configure
      cluster:
        type: choice
        description: Select cluster
        options:
        - staging
        - production
        - staging and production
        default: staging
env:
  RELEASES_REPOSITORY: $GITHUB_REPOSITORY_OWNER/releases

# this ensures that only one CI pipeline with the same key
#  can run at once in order to prevent undefined states
concurrency: cluster-update-mutex

jobs:
  setup:
    name: Check secrets and set up matrix
    runs-on: ubuntu-20.04
    strategy:
      matrix:
        environment-name: [Biomage]
    environment: ${{ matrix.environment-name }}
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - id: check-secrets
        name: Check if necessary secrets are installed.
        run: |-
          echo Checking if secrets are defined in the repository.

          if [ -z "${{ secrets.ACM_CERTIFICATE_ARN}}" ]
          then
            echo AWS certificate ARN is not defined.
            ERROR=true
          fi

          if [ -z "${{ secrets.AWS_ACCESS_KEY_ID }}" ]
          then
            echo AWS Access Key ID not defined.
            ERROR=true
          fi

          if [ -z "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ]
          then
            echo AWS Secret Access Key not defined.
            ERROR=true
          fi

          if [ -z "${{ secrets.API_TOKEN_GITHUB }}" ]
          then
            echo GitHub deploy key access token not defined.
            ERROR=true
          fi

          if [ -z "${{ secrets.PRIMARY_DOMAIN_NAME }}" ]
          then
            echo Secret PRIMARY_DOMAIN_NAME is not set in repository secrets. Make sure this secret exists in the repository secrets.
            ERROR=true
          fi

          if [ -z "${{ secrets.DOMAIN_NAME }}" ]
          then
            echo Secret DOMAIN_NAME is not set in repository secrets. Make sure this secret exists in the repository secrets.
            ERROR=true
          fi

          if [ ! -z "$ERROR" ]
          then
            echo
            echo This workflow requires some secrets to complete.
            echo Please make they are created by adding/rotating them manually.
            exit 1
          fi
      - id: set-matrix
        name: Set up cluster matrix
        run: |-
          if [ "${CLUSTER}" = "staging" ]; then
            echo '::set-output name=matrix::["staging"]'
          elif [ "${CLUSTER}" = "production" ]; then
            echo '::set-output name=matrix::["production"]'
          elif [ "${CLUSTER}" = "staging and production" ]; then
            echo '::set-output name=matrix::["staging", "production"]'
          fi
        env:
          CLUSTER: ${{ github.event.inputs.cluster }}

  create-eks-cluster:
    name: Create EKS cluster
    runs-on: ubuntu-20.04
    needs: setup
    env:
      CLUSTER_ENV: ${{ matrix.environment-type }}
    strategy:
      max-parallel: 1
      matrix:
        environment-type: ${{ fromJson(needs.setup.outputs.matrix) }}
        environment-name: [Biomage]
    environment: ${{ matrix.environment-name }}
    if: github.event.inputs.workflow_actions == 'deploy and configure'
    steps:
      - id: checkout
        name: Check out source code
        uses: actions/checkout@v2

      - id: setup-aws
        name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - id: install-yq
        name: Install yq for modifying the eksctl yaml spec.
        run: |-
          sudo wget https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64 -O /usr/bin/yq && sudo chmod +x /usr/bin/yq

      - id: fill-metadata
        name: Add name and region to the eksctl file.
        run: |-
          yq w infra/cluster.yaml metadata.name "biomage-$CLUSTER_ENV" | yq w - metadata.region ${{ secrets.AWS_REGION }} > /tmp/cluster-$CLUSTER_ENV.yaml
          cat /tmp/cluster-$CLUSTER_ENV.yaml

      - id: install-eksctl
        name: Install eksctl
        run: |-
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin

      - id: create-clusters
        name: Attempt to create clusters from spec.
        # this job will always pass, irrespective of whether creation was successful or not.
        # this is because the cluster may already exist. we will check for this condition
        # on failure in the next step
        continue-on-error: true
        run: |-
          exec &> >(tee /tmp/eksctl-$CLUSTER_ENV.log)

          eksctl create cluster -f /tmp/cluster-$CLUSTER_ENV.yaml
          echo "::set-output name=outcome::created"
      - id: check-for-failure
        name: Check for reason of failure if cluster creation failed.
        if: steps.create-clusters.outcome == 'failure'
        run: |-
          # Check if failure was caused by an already exists exception.
          # If not, the job should fail.
          ALREADY_EXISTS=$(grep AlreadyExistsException /tmp/eksctl-$CLUSTER_ENV.log | wc -l | xargs)
          if [ $ALREADY_EXISTS -ne 1 ]
          then
            echo Step failed for reason other than stack already existing.
            echo Job failing...
            echo "::set-output name=reason::error"
            false
          fi

          echo Cluster already exists.
          echo "::set-output name=reason::already-exists"

      - id: update-nodegroup
        name: Attempt to update node groups for existing cluster.
        if: steps.create-clusters.outcome == 'failure' && steps.check-for-failure.outputs.reason == 'already-exists'
        run: |-
          eksctl create nodegroup --config-file=/tmp/cluster-$CLUSTER_ENV.yaml
          eksctl delete nodegroup --config-file /tmp/cluster-$CLUSTER_ENV.yaml --only-missing --approve

      # note: iam service accounts should really be created from within the helm chart as seen here:
      # https://docs.aws.amazon.com/eks/latest/userguide/specify-service-account-role.html
      - id: update-serviceaccounts
        name: Attempt to update IAM service accounts for existing cluster.
        if: steps.create-clusters.outcome == 'failure' && steps.check-for-failure.outputs.reason == 'already-exists'
        run: |-
          eksctl utils associate-iam-oidc-provider --config-file=/tmp/cluster-$CLUSTER_ENV.yaml --approve
          eksctl create iamserviceaccount --config-file=/tmp/cluster-$CLUSTER_ENV.yaml
          eksctl delete iamserviceaccount --config-file=/tmp/cluster-$CLUSTER_ENV.yaml --only-missing --approve

  configure-cluster:
    name: Configure Kubernetes resources on the EKS cluster
    runs-on: ubuntu-20.04
    needs: [setup, create-eks-cluster]
    if: always() && (needs.create-eks-cluster.result == 'success' || needs.create-eks-cluster.result == 'skipped')
    env:
      CLUSTER_ENV: ${{ matrix.environment-type }}
      API_TOKEN_GITHUB: ${{ secrets.API_TOKEN_GITHUB }}
    strategy:
      max-parallel: 1
      matrix:
        environment-type: ${{ fromJson(needs.setup.outputs.matrix) }}
        environment-name: [Biomage]
    environment: ${{ matrix.environment-name }}
    steps:
      - id: checkout
        name: Check out source code
        uses: actions/checkout@v2

      - id: setup-aws
        name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - id: add-kubeconfig
        name: Add k8s config file for existing cluster.
        run: |-
          aws eks update-kubeconfig --name biomage-$CLUSTER_ENV

      - id: deploy-metrics-server
        name: Deploy k8s metrics server
        run: |-
          kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

      - id: create-flux-namespace
        name: Attempt to create flux namespace
        continue-on-error: true
        run: |-
          kubectl create namespace flux

      - id: setup-domain
        name: Compile environment-specific domain name
        run: |-
          if [ "${{ matrix.environment-type }}" = "production" ]; then
            PRIMARY_DOMAIN_NAME="${{ secrets.PRIMARY_DOMAIN_NAME }}"
            DOMAIN_NAME="${{ secrets.DOMAIN_NAME }}"
          fi
          if [ "${{ matrix.environment-type }}" = "staging" ]; then
            PRIMARY_DOMAIN_NAME="${{ secrets.PRIMARY_DOMAIN_NAME }}"
            DOMAIN_NAME="${{ secrets.DOMAIN_NAME_STAGING }}"
          fi
          echo "::set-output name=primary-domain-name::$PRIMARY_DOMAIN_NAME"
          echo "::set-output name=domain-name::$DOMAIN_NAME"

      - id: install-yq
        name: Install yq for modifying the deployment spec.
        run: |-
          sudo wget https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64 -O /usr/local/bin/yq && sudo chmod +x /usr/local/bin/yq

      - id: platform-monitoring-enabled
        name: Get config for whether monitoring should be enabled for deployment
        uses: mikefarah/yq@master
        with:
          cmd: yq '.[env(ENVIRONMENT_NAME)].monitoringEnabled' 'infra/config/github-environments-config.yaml'
        env:
          ENVIRONMENT_NAME: ${{ matrix.environment-name }}

      - id: fill-account-specific-metadata
        name: Fill in account specific metadata in ConfigMap
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_ACCOUNT_ID: ${{ steps.setup-aws.outputs.aws-account-id }}
          DOMAIN_NAME: ${{ steps.setup-domain.outputs.domain-name }}
          ENVIRONMENT_NAME: ${{ matrix.environment-name }}
          MONITORING_ENABLED: ${{ steps.platform-monitoring-enabled.outputs.result }}
        run: |-
          CONFIG_FILE="infra/config/account-config.yaml"

          yq w -d0 -i $CONFIG_FILE myAccount.domainName --style=double "$DOMAIN_NAME"
          yq w -d0 -i $CONFIG_FILE myAccount.region --style=double "$AWS_REGION"
          yq w -d0 -i $CONFIG_FILE myAccount.accountId --style=double "$AWS_ACCOUNT_ID"
          yq w -d0 -i $CONFIG_FILE myAccount.acmCertificate --style=double "${{ secrets.ACM_CERTIFICATE_ARN }}"
          yq w -d0 -i $CONFIG_FILE myAccount.datadogEnabled --style=double "false"
          yq w -d0 -i $CONFIG_FILE myAccount.datadogApiKey --style=double ""

          if [[ $MONITORING_ENABLED = "true" ]]
          then
            yq w -d0 -i $CONFIG_FILE myAccount.datadogEnabled --style=double "true"
            yq w -d0 -i $CONFIG_FILE myAccount.datadogApiKey --style=double "${{ secrets.DATADOG_API_KEY }}"
          fi

          cat $CONFIG_FILE

      - id: create-account-information-configmap
        name: Create a configmap containing AWS account specific details
        continue-on-error: false
        run: |-
          kubectl create configmap account-config --from-file=infra/config/account-config.yaml -n flux -o yaml --dry-run | kubectl apply -f -

      - id: install-fluxctl
        name: Install fluxctl
        run: |-
          sudo snap install fluxctl --classic

      - id: install-helm
        name: Install Helm
        run: |-
          sudo snap install helm --classic

      - id: install-eksctl
        name: Install eksctl
        run: |-
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin

      - id: deploy-load-balancer-role
        name: Deploy permissions for AWS load balancer controller
        run: |-
          curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.0/docs/install/iam_policy.json

          aws iam create-policy \
            --policy-name AWSLoadBalancerControllerIAMPolicy-$CLUSTER_ENV \
            --policy-document file://iam-policy.json || true

          eksctl create iamserviceaccount \
            --cluster=biomage-$CLUSTER_ENV \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --attach-policy-arn=arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:policy/AWSLoadBalancerControllerIAMPolicy-$CLUSTER_ENV \
            --override-existing-serviceaccounts \
            --approve

      # we need to retry this due to an active issue with the AWS Load Balancer Controller
      # where there are intermittent failures that are only fixable by retrying
      # see issue at https://github.com/kubernetes-sigs/aws-load-balancer-controller/issues/2071
      - id: install-lbc
        name: Deploy AWS Load Balancer Controller
        uses: nick-invision/retry@v2
        with:
          timeout_seconds: 600
          max_attempts: 20
          retry_on: error
          on_retry_command: sleep $(shuf -i 5-15 -n 1)
          command: |-
            helm repo add eks https://aws.github.io/eks-charts
            kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master"

            helm repo update

            helm upgrade aws-load-balancer-controller eks/aws-load-balancer-controller \
              --namespace kube-system \
              --set serviceAccount.create=false \
              --set serviceAccount.name=aws-load-balancer-controller \
              --set clusterName=biomage-$CLUSTER_ENV \
              --install --wait

            # this is needed so SNS does not stop trying to subscribe to not-yet-deployed
            # API staging environments because their endpoints are not yet available.
            helm upgrade aws-elb-503-subscription-endpoint infra/aws-elb-503-subscription-endpoint \
              --namespace default \
              --set clusterEnv=$CLUSTER_ENV \
              --set acmCertificate=${{ secrets.ACM_CERTIFICATE_ARN }} \
              --install --wait

      - id: platform-public-facing
        name: Get config for whether platform should be public facing
        uses: mikefarah/yq@master
        with:
          cmd: yq '.[env(ENVIRONMENT_NAME)].publicFacing' 'infra/config/github-environments-config.yaml'
        env:
          ENVIRONMENT_NAME: ${{ matrix.environment-name }}

      - id: deploy-env-loadbalancer
        name: Deploy AWS Application Load Balancer for environment
        uses: aws-actions/aws-cloudformation-github-deploy@v1
        with:
          parameter-overrides: "Environment=${{ matrix.environment-type }},PublicFacing=${{ steps.platform-public-facing.outputs.result }}"
          name: "biomage-k8s-alb-${{ matrix.environment-type }}"
          template: 'infra/cf-loadbalancer.yaml'
          no-fail-on-empty-changeset: "1"

      - id: deploy-route53
        name: Deploy Route 53 DNS records to ELB
        uses: aws-actions/aws-cloudformation-github-deploy@v1
        with:
          parameter-overrides: "Environment=${{ matrix.environment-type }},DNSName=${{ steps.deploy-env-loadbalancer.outputs.DNSName }},HostedZoneId=${{ steps.deploy-env-loadbalancer.outputs.CanonicalHostedZoneID }},PrimaryDomainName=${{ steps.setup-domain.outputs.primary-domain-name }},DomainName=${{ steps.setup-domain.outputs.domain-name }}"
          name: "biomage-alb-route53-${{ matrix.environment-type }}"
          template: 'infra/cf-route53.yaml'
          no-fail-on-empty-changeset: "1"

      - id: flux-git-read-only
        name: Get Flux mode
        uses: mikefarah/yq@master
        with:
          cmd: yq '.[env(ENVIRONMENT_NAME)].fluxGitReadOnly' 'infra/config/github-environments-config.yaml'
        env:
          ENVIRONMENT_NAME: ${{ matrix.environment-name }}

      - id: install-flux
        name: Deploy flux
        run: |-
          helm repo add fluxcd https://charts.fluxcd.io
          kubectl apply -f https://raw.githubusercontent.com/fluxcd/helm-operator/master/deploy/crds.yaml

          FLUX_GIT_READ_ONLY=${{ steps.flux-git-read-only.outputs.result }}

          if [[ $FLUX_GIT_READ_ONLY = "false" ]]; then
            GIT_URL=git@github.com:${{ env.RELEASES_REPOSITORY }}
          elif [[ $FLUX_GIT_READ_ONLY = "true" ]]; then
            GIT_URL=https://github.com/${{ env.RELEASES_REPOSITORY }}
          fi

          helm upgrade flux fluxcd/flux \
            --set git.url=$GIT_URL \
            --set git.path="$CLUSTER_ENV" \
            --set git.label="flux-sync-$CLUSTER_ENV" \
            --set git.pollInterval="2m" \
            --set git.timeout="40s" \
            --set git.readonly=$FLUX_GIT_READ_ONLY \
            --set syncGarbageCollection.enabled=true \
            --namespace flux \
            --install --wait

          helm upgrade helm-operator fluxcd/helm-operator \
            --set git.ssh.secretName=flux-git-deploy \
            --set helm.versions=v3 \
            --set git.pollInterval="2m" \
            --set git.timeout="40s" \
            --namespace flux \
            --install --wait
        env:
          ENVIRONMENT_NAME: ${{ matrix.environment-name }}

      - id: get-public-key
        name: Get flux SSH deploy key
        if: steps.flux-git-read-only.outputs.result == 'false'
        run: |-
          fluxctl identity --k8s-fwd-ns flux | tee ~/flux.pub

      - id: add-key-to-github
        name: Add Flux deploy key to GitHub repository
        if: steps.flux-git-read-only.outputs.result == 'false'
        run: |-
          # find existing key IDs. save them to a file
          curl \
            -H"Authorization: token $API_TOKEN_GITHUB"\
            https://api.github.com/repos/${{ env.RELEASES_REPOSITORY }}/keys 2>/dev/null\
            | jq '.[] | select(.title | contains(env.CLUSTER_ENV)) | .id' > /tmp/key_ids

          # iterate through them and delete all existing deploy keys
          cat /tmp/key_ids | \
            while read _id; do
              echo "- delete  deploy key: $_id"
              curl \
                -X "DELETE"\
                -H"Authorization: token $API_TOKEN_GITHUB"\
                https://api.github.com/repos/${{ env.RELEASES_REPOSITORY }}/keys/$_id 2>/dev/null
            done

          # add the keyfile to github
          echo
          echo "+ flux deploy key:"
          echo -n ">> "
          {
            curl \
              -i\
              -H"Authorization: token $API_TOKEN_GITHUB"\
              --data @- https://api.github.com/repos/${{ env.RELEASES_REPOSITORY }}/keys << EOF
            {
              "title" : "Flux CI -- $CLUSTER_ENV -- $(date)",
              "key" : "$(cat ~/flux.pub)",
              "read_only" : false
            }
          EOF
          } 2>/dev/null | tee /tmp/create_key_result | head -1

          # check if key was created
          KEY_CREATED=$(grep 201 /tmp/create_key_result | wc -l | xargs)
          if [ $KEY_CREATED -eq 0 ]
          then
            echo
            echo Key creation failed. Full response shown below:
            cat /tmp/create_key_result
            false
          fi

      - id: deploy-xray-daemon
        name: Deploy AWS X-Ray daemon
        run: |-
          helm upgrade "aws-xray-daemon" infra/aws-xray-daemon \
            --namespace default \
            --set iamRole=arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:role/xray-daemon-role-$CLUSTER_ENV \
            --install --wait

      - id: install-ebs-csi-driver
        name: Install AWS EBS Container Storage Interface (CSI) drivers
        run: |-
          helm upgrade \
            aws-ebs-csi-driver https://github.com/kubernetes-sigs/aws-ebs-csi-driver/releases/download/helm-chart-aws-ebs-csi-driver-2.6.4/aws-ebs-csi-driver-2.6.4.tgz \
            --namespace kube-system \
            --set enableVolumeScheduling=true \
            --set enableVolumeResizing=true \
            --set enableVolumeSnapshot=true \
            --install --wait \

      - id: deploy-read-only-group
        name: Deploy read-only permission definition for cluster
        run: |-
          helm upgrade "biomage-read-only-group" infra/biomage-read-only-group \
            --install --wait

      - id: deploy-state-machine-role
        name: Deploy AWS Step Function (state machine) roles
        uses: aws-actions/aws-cloudformation-github-deploy@v1
        with:
          parameter-overrides: "Environment=${{ matrix.environment-type }}"
          name: "biomage-state-machine-role-${{ matrix.environment-type }}"
          template: 'infra/cf-state-machine-role.yaml'
          capabilities: 'CAPABILITY_IAM,CAPABILITY_NAMED_IAM'
          no-fail-on-empty-changeset: "1"

      - id: remove-identitymappings
        name: Remove all previous identity mappings for IAM users
        run: |-
          eksctl get iamidentitymapping --cluster=biomage-$CLUSTER_ENV --output=json | \
          jq -r '.[] | select(.userarn != null) | .userarn' > /tmp/users_to_remove

          while IFS= read -r user
          do
            echo "Remove rights of $user"
            eksctl delete iamidentitymapping \
              --cluster=biomage-$CLUSTER_ENV \
              --arn $user \
              --all
          done < "/tmp/users_to_remove"

      # see https://eksctl.io/usage/iam-identity-mappings/
      - id: add-state-machine-role
        name: Grant rights to the state machine IAM role.
        run: |-
          eksctl create iamidentitymapping \
            --cluster=biomage-$CLUSTER_ENV \
            --arn arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:role/state-machine-role-$CLUSTER_ENV \
            --group state-machine-runner-group \
            --username state-machine-runner

      # see https://eksctl.io/usage/iam-identity-mappings/
      - id: update-identitymapping-admin
        name: Add cluster admin rights to everyone on the admin list.
        run: |-
          echo "Reading cluster rights from file: infra/cluster-access/${{matrix.environment-name}}/cluster_admins_$CLUSTER_ENV"
          while IFS= read -r user
          do
            echo "Adding cluster admin rights to $user"
            eksctl create iamidentitymapping \
              --cluster=biomage-$CLUSTER_ENV \
              --arn arn:aws:iam::${{ steps.setup-aws.outputs.aws-account-id }}:user/$user \
              --group system:masters \
              --username $user
          done < "infra/cluster-access/${{matrix.environment-name}}/cluster_admins_$CLUSTER_ENV"

      - id: setup-cluster-cloudwatch-logging-policy
        name: Setup permissions to cluster to log to Cloudwatch
        if: matrix.environment-name == 'Biomage'
        env:
          AWS_ACCOUNT_ID: ${{ steps.setup-aws.outputs.aws-account-id }}
        run: |-
          LOGGING_POLICY_NAME="cluster-cloudwatch-log-policy"

          aws iam create-policy \
          --policy-name $LOGGING_POLICY_NAME \
          --policy-document infra/cluster-logging/cluster-cloudwatch-log-policy.json || true

          echo "::set-output name=logging-policy-name::$LOGGING_POLICY_NAME"

      - id: setup-fargate-logging
        name: Setup logging for Fargate pods
        if: matrix.environment-name == 'Biomage'
        env:
          AWS_ACCOUNT_ID: ${{ steps.setup-aws.outputs.aws-account-id }}
          LOGGING_POLICY_NAME: ${{ steps.setup-cluster-cloudwatch-logging-policy.outputs.logging-policy-name }}
        run: |-
          # Create FluentBit config
          sed -i 's/CLUSTER_ENV/${CLUSTER_ENV}/g' fargate-fluentbit-config.yaml > fargate-fluentbit-config.yaml
          kubectl apply -f infra/fargate-logging/fargate-fluentbit-config.yaml

          # Attach role for pipeline and worker pods
          PIPELINE_POD_EXEC_ROLE=$(aws eks describe-fargate-profile \
            --cluster-name biomage-$CLUSTER_ENV \
            --fargate-profile-name pipeline-default | jq -r '.fargateProfile.podExecutionRoleArn' | awk -F"/" '{print (NF>1)? $NF : ""}' )
          aws iam attach-role-policy \
          --policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/$LOGGING_POLICY_NAME \
          --role-name $PIPELINE_POD_EXEC_ROLE

          WORKER_POD_EXEC_ROLE=$(aws eks describe-fargate-profile \
            --cluster-name biomage-$CLUSTER_ENV \
            --fargate-profile-name worker-default | jq -r '.fargateProfile.podExecutionRoleArn' | awk -F"/" '{print (NF>1)? $NF : ""}' )
          aws iam attach-role-policy \
          --policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/$LOGGING_POLICY_NAME \
          --role-name $WORKER_POD_EXEC_ROLE

      - id: setup-datadog-cluster-agent
        name: Setup Datadog cluster agent
        if: ${{ steps.platform-monitoring-enabled.outputs.result == 'true'}}
        run: |-
          helm repo add datadog https://helm.datadoghq.com
          helm repo update
          helm upgrade datadog-agent datadog/datadog \
            -f infra/datadog/cluster-agent-values.yaml \
            --set datadog.apiKey=${{ secrets.DATADOG_API_KEY }} \
            --set datadog.clusterName=biomage-$CLUSTER_ENV \
            --install

      - id: setup-datadog-sidecar-permissions
        name: Setup Datadog sidecar permissions
        if: ${{ steps.platform-monitoring-enabled.outputs.result == 'true'}}
        run: |-
          kubectl apply -f infra/datadog/datadog-sidecar-rbac.yaml

  report-if-failed:
    name: Report if workflow failed
    runs-on: ubuntu-20.04
    needs: [setup, create-eks-cluster, configure-cluster]
    if: failure() && github.ref == 'refs/heads/master'
    steps:
      - id: send-to-slack
        name: Send failure notification to Slack on failure
        env:
          SLACK_BOT_TOKEN: ${{ secrets.BUILD_STATUS_BOT_TOKEN }}
        uses: voxmedia/github-action-slack-notify-build@v1
        with:
          channel: pipelines
          status: FAILED
          color: danger
